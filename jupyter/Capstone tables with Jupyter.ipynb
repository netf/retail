{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Welcome to This Jupyter example\n",
    "You simply type stuff in the \"In\" boxes, and it the play button.  You can also it shift-enter to play the box.  The cell menu lets you click multiple boxes.   This sort of thing can be the future of our self-paced spark spark training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val x = sc.cassandraTable(\"retail\",\"products_by_id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "receipt_id bigint,\n",
    "    scan_id timeuuid,\n",
    "    credit_card_number bigint static,\n",
    "    credit_card_type text static,\n",
    "    product_id text,\n",
    "    product_name text,\n",
    "    quantity int,\n",
    "    receipt_timestamp timestamp static,\n",
    "    receipt_total decimal static,\n",
    "    register_id int static,\n",
    "    solr_query text,\n",
    "    store_id int static,\n",
    "    total decimal,\n",
    "    unit_price decimal,\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "case class Receipts(\n",
    "    receipt_id: String,\n",
    "    scan_id: String,\n",
    "    credit_card_number: Option[String],\n",
    "    credit_card_type: Option[String],\n",
    "    receipt_timestamp: Option[String],\n",
    "    receipt_total: Option[String],\n",
    "    store_id: Option[String]\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " scan_id timeuuid,\n",
    "    receipt_timestamp timestamp,\n",
    "    receipt_total decimal,\n",
    "    store_id int,\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val receipts = sc.cassandraTable(\"retail\",\"receipts\").select(\"receipt_id\",\"credit_card_number\",\"scan_id\",\"receipt_timestamp\",\"receipt_total\",\"store_id\").as(Receipts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Long = 351677"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "receipts.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array[(String, String, String, String, String, String)] = Array((1444803402917,4716605562641,23998a14-723b-11e5-b4bd-8b496c707234,2015-02-01 17:30:36+0000,2499.77,393), (1444802830615,373994855939263,ce93e753-7239-11e5-b4bd-8b496c707234,2015-02-01 19:30:29+0000,1329.8,227), (1444691779674,5310231762049760,3f4b7d45-7137-11e5-9cc1-8b496c707234,2015-02-02 01:23:49+0000,1989.72,154), (1444798582375,5579704434928179,ea4380f2-722f-11e5-b4bd-8b496c707234,2015-02-01 23:12:02+0000,1609.72,368), (1444802771407,4916549559527,ab5f9ae7-7239-11e5-b4bd-8b496c707234,2015-02-02 02:41:45+0000,619.83,123), (1444691684673,371964228082366,065ff4c3-7137-11e5-9cc1-8b496c707234,2015-02-01 16:58:09+0000,999.77,223), (1444691209220,4556768116938,eb367c12-7135-11e5-9cc1-8b496c707234,2015-02-01 23:57:31+00..."
      ]
     },
     "execution_count": 316,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_credit_card.take(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "case class Customers(\n",
    "    id: Option[String]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val customers = sc.cassandraTable(\"retail\",\"customers\").select(\"id\").map(x => x.getString(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val customers_with_key = customers.zipWithIndex().map(_.swap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0,dbc3347c-b5e2-4dda-b13d-47c3dc627ad0)\n",
      "(1,a8e06888-519f-4a74-8bad-06e0dfbc7183)\n",
      "(2,8b427fb1-1649-4d68-91ac-5a487fac0f6f)\n",
      "(3,d17b4671-5034-456b-9843-906e70a63dd6)\n",
      "(4,a2032764-a809-4338-8c10-94e6afd89e20)\n",
      "(5,e05ce9bf-aaa8-4f49-bc4f-6c9b8a688cc1)\n",
      "(6,45140371-5ea7-4392-aa66-b8d69f001d84)\n",
      "(7,f7596900-68a9-4452-a5a4-a2f811b3a755)\n",
      "(8,4d743b08-9f06-4d29-a344-1955e4f11d8a)\n",
      "(9,737a3465-8655-40c8-8df8-969e08f11f4f)\n"
     ]
    }
   ],
   "source": [
    "customers_with_key.take(10).foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x.take(2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "make a link between customer and receipt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val receipts_with_key = sc.cassandraTable(\"retail\",\"receipts\").select(\"receipt_id\",\"scan_id\",\"credit_card_number\",\"credit_card_type\",\"receipt_timestamp\",\"receipt_total\",\"store_id\").as(Receipts).map(x => (((x.receipt_id + x.scan_id).hashCode() % 5000).abs.toLong, (x.receipt_id, x.scan_id, x.credit_card_number, x.credit_card_type, x.receipt_timestamp, x.receipt_total, x.store_id)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2397,(1444798652898,14506d47-7230-11e5-b4bd-8b496c707234,Some(372718046963499),Some(American Express),Some(2015-02-01 19:55:15+0000),Some(1249.8500000000001),Some(247)))\n",
      "(4931,(1444798652898,145c5421-7230-11e5-b4bd-8b496c707234,Some(372718046963499),Some(American Express),Some(2015-02-01 19:55:15+0000),Some(1249.8500000000001),Some(247)))\n",
      "(2906,(1444798652898,14697380-7230-11e5-b4bd-8b496c707234,Some(372718046963499),Some(American Express),Some(2015-02-01 19:55:15+0000),Some(1249.8500000000001),Some(247)))\n",
      "(1923,(1444798652898,14881f1b-7230-11e5-b4bd-8b496c707234,Some(372718046963499),Some(American Express),Some(2015-02-01 19:55:15+0000),Some(1249.8500000000001),Some(247)))\n",
      "(2519,(1444798638448,0bb44b71-7230-11e5-b4bd-8b496c707234,Some(5150065054555248),Some(MasterCard),Some(2015-02-01 22:03:46+0000),Some(919.9200000000001),Some(390)))\n",
      "(3741,(1444798638448,0bc08078-7230-11e5-b4bd-8b496c707234,Some(5150065054555248),Some(MasterCard),Some(2015-02-01 22:03:46+0000),Some(919.9200000000001),Some(390)))\n",
      "(2060,(1444798638448,0bcb55e3-7230-11e5-b4bd-8b496c707234,Some(5150065054555248),Some(MasterCard),Some(2015-02-01 22:03:46+0000),Some(919.9200000000001),Some(390)))\n",
      "(106,(1444691285048,18165e82-7136-11e5-9cc1-8b496c707234,Some(347215686985223),Some(American Express),Some(2015-02-03 01:53:13+0000),Some(149.95),Some(188)))\n",
      "(1205,(1444853412094,93532092-72af-11e5-b4bd-8b496c707234,Some(4485897689120),Some(Visa),Some(2015-02-02 00:37:49+0000),Some(849.9000000000001),Some(163)))\n",
      "(2319,(1444853412094,93591402-72af-11e5-b4bd-8b496c707234,Some(4485897689120),Some(Visa),Some(2015-02-02 00:37:49+0000),Some(849.9000000000001),Some(163)))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Long = 351677"
      ]
     },
     "execution_count": 322,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "receipts_with_key.take(10).foreach(println)\n",
    "receipts_with_key.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val receipts_by_customer = receipts_with_key.join(customers_with_key).map{ case (x,y) => y}.map(_.swap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some Case Class Example with some markdown\n",
    "Lets look at the stores table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Long = 351677"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "receipts_by_customer.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: org.apache.spark.SparkException\n",
       "Message: Job aborted due to stage failure: Task 1 in stage 263.0 failed 1 times, most recent failure: Lost task 1.0 in stage 263.0 (TID 309, localhost): java.io.IOException: Failed to write statements to retail.receipts_by_customer.\n",
       "\tat com.datastax.spark.connector.writer.TableWriter$$anonfun$write$1.apply(TableWriter.scala:164)\n",
       "\tat com.datastax.spark.connector.writer.TableWriter$$anonfun$write$1.apply(TableWriter.scala:139)\n",
       "\tat com.datastax.spark.connector.cql.CassandraConnector$$anonfun$withSessionDo$1.apply(CassandraConnector.scala:110)\n",
       "\tat com.datastax.spark.connector.cql.CassandraConnector$$anonfun$withSessionDo$1.apply(CassandraConnector.scala:109)\n",
       "\tat com.datastax.spark.connector.cql.CassandraConnector.closeResourceAfterUse(CassandraConnector.scala:139)\n",
       "\tat com.datastax.spark.connector.cql.CassandraConnector.withSessionDo(CassandraConnector.scala:109)\n",
       "\tat com.datastax.spark.connector.writer.TableWriter.write(TableWriter.scala:139)\n",
       "\tat com.datastax.spark.connector.RDDFunctions$$anonfun$saveToCassandra$1.apply(RDDFunctions.scala:37)\n",
       "\tat com.datastax.spark.connector.RDDFunctions$$anonfun$saveToCassandra$1.apply(RDDFunctions.scala:37)\n",
       "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:63)\n",
       "\tat org.apache.spark.scheduler.Task.run(Task.scala:70)\n",
       "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:213)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n",
       "\tat java.lang.Thread.run(Thread.java:745)\n",
       "\n",
       "Driver stacktrace:\n",
       "StackTrace: org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1273)\n",
       "org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1264)\n",
       "org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1263)\n",
       "scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n",
       "scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)\n",
       "org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1263)\n",
       "org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:730)\n",
       "org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:730)\n",
       "scala.Option.foreach(Option.scala:236)\n",
       "org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:730)\n",
       "org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1457)\n",
       "org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1418)\n",
       "org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)"
      ]
     },
     "execution_count": 324,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "receipts_by_customer.map{case (k,(a,b,c,d,e,f,g)) => (k,a,b,c,d,e,f,g)}.saveToCassandra(\"retail\",\"receipts_by_customer\", SomeColumns(\"customer_id\",\"receipt_id\",\"scan_id\",\"credit_card_number\",\"credit_card_type\",\"receipt_timestamp\",\"receipt_total\",\"store_id\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "case class R (\n",
    "    customer_id: String,\n",
    "    receipt_id: String,\n",
    "    store_id: String,\n",
    "    receipt_total: Option[Float]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val customer_receipt_store = sc.cassandraTable(\"retail\",\"receipts_by_customer\").select(\"customer_id\",\"receipt_id\",\"store_id\",\"total\").as(R)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "val top_customers = customer_receipt_store.map(x => (x.customer_id, x.receipt_id, x.store_id)).map{case (a,b,c) => (a,(c,1))}.groupByKey()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val receipts_by_customers = sc.cassandraTable(\"retail\",\"receipts_by_customer\").select(\"customer_id\",\"receipt_id\",\"store_id\",\"receipt_total\").as(R)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "val receipts_by_customers_c = receipts_by_customer.filter(x => x.receipt_total != null)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val total = receipts_by_customers.map( x => ((x.store_id, x.customer_id), x.receipt_total.getOrElse(0.0f))).reduceByKey(_+_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val total2 = total.map{ case ((store_id, customer_id), total) => (store_id, (customer_id, total)) }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(String, Iterable[(String, Float)]) = (273,CompactBuffer((eb6e2e45-088d-46e4-9fed-7428285acad8,939.87), (b34bbd01-4052-4bf7-9cfe-95f6571cb79b,909.85), (88fdc174-0fbc-4fd0-a37b-e5808fccdf26,1309.88), (da77aac3-42e1-4315-89da-63f9f21a0095,1589.86), (eacbd77e-3db4-4e7c-b20e-2fdceb83d541,2779.66), (940bc211-a8fe-4d82-add9-a8f55441a70a,1429.86), (014d8728-1465-4c38-abad-97e828b21d44,2779.66), (4728df9c-2ee6-4142-8133-23c8dc848e19,1259.77), (f3a78056-5b6b-4134-a7df-a95c87593e6a,209.93), (02180274-4a21-406b-96c0-ee75c1ba72cb,1189.9), (420f7355-b949-468d-8b0f-4692e34a7957,999.82), (1b681a02-ac4e-43ef-8476-fd6bcba469fb,919.88), (0f1a3f96-424d-45ce-8e0a-ceb4e68592a6,2639.67), (2eef0c99-c4ab-4709-ab9c-a154d292c1a0,1049.87), (2a622002-7122-4615-a8d7-6c6e00f24123,529.87), (787fad57-a49c-4d43..."
      ]
     },
     "execution_count": 438,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total2.groupByKey().take(4)(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: org.apache.spark.SparkException\n",
       "Message: Job aborted due to stage failure: Task 0 in stage 295.0 failed 1 times, most recent failure: Lost task 0.0 in stage 295.0 (TID 345, localhost): scala.MatchError: UUIDType (of class org.apache.spark.sql.cassandra.types.UUIDType$)\n",
       "\tat org.apache.spark.sql.execution.SparkSqlSerializer2$$anonfun$createSerializationFunction$1.apply(SparkSqlSerializer2.scala:232)\n",
       "\tat org.apache.spark.sql.execution.SparkSqlSerializer2$$anonfun$createSerializationFunction$1.apply(SparkSqlSerializer2.scala:227)\n",
       "\tat org.apache.spark.sql.execution.Serializer2SerializationStream.writeValue(SparkSqlSerializer2.scala:70)\n",
       "\tat org.apache.spark.storage.DiskBlockObjectWriter.write(BlockObjectWriter.scala:207)\n",
       "\tat org.apache.spark.util.collection.WritablePartitionedIterator$$anon$3.writeNext(WritablePartitionedPairCollection.scala:104)\n",
       "\tat org.apache.spark.util.collection.ExternalSorter.spillToPartitionFiles(ExternalSorter.scala:375)\n",
       "\tat org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:208)\n",
       "\tat org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:62)\n",
       "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:70)\n",
       "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)\n",
       "\tat org.apache.spark.scheduler.Task.run(Task.scala:70)\n",
       "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:213)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n",
       "\tat java.lang.Thread.run(Thread.java:745)\n",
       "\n",
       "Driver stacktrace:\n",
       "StackTrace: org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1273)\n",
       "org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1264)\n",
       "org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1263)\n",
       "scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n",
       "scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)\n",
       "org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1263)\n",
       "org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:730)\n",
       "org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:730)\n",
       "scala.Option.foreach(Option.scala:236)\n",
       "org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:730)\n",
       "org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1457)\n",
       "org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1418)\n",
       "org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)"
      ]
     },
     "execution_count": 359,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.SQLContext\n",
    "\n",
    "val sqlContext = new SQLContext(sc)\n",
    "\n",
    "val receipts_df = sqlContext.read.format(\"org.apache.spark.sql.cassandra\").\n",
    "    options(Map(\"keyspace\"-> \"retail\", \"table\" -> \"receipts\")).\n",
    "    load()\n",
    "receipts_df.registerTempTable(\"receipts\")\n",
    "\n",
    "val receipts_by_customer = sqlContext.read.format(\"org.apache.spark.sql.cassandra\").\n",
    "    options(Map(\"keyspace\"-> \"retail\", \"table\" -> \"receipts_by_customer\")).\n",
    "    load()\n",
    "    \n",
    "    \n",
    "receipts_by_customer.registerTempTable(\"receipts_by_customer\")\n",
    "\n",
    "val customers = sqlContext.read.format(\"org.apache.spark.sql.cassandra\").\n",
    "    options(Map(\"keyspace\"-> \"retail\", \"table\" -> \"customers\")).\n",
    "    load()\n",
    "    \n",
    "    \n",
    "customers.registerTempTable(\"customers\")\n",
    "\n",
    "val x = sqlContext.sql(\"select c.firstname, c.lastname, r.store_id, sum(r.total) \" +\n",
    "    \" from receipts r,receipts_by_customer rc, customers c \"+\n",
    "    \" where r.receipt_id = rc.receipt_id and rc.customer_id = c.id group by c.firstname,c.lastname,r.store_id\")\n",
    "x.show\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "case class Store(\n",
    "                  store_id: Int,\n",
    "                  address: String,\n",
    "                  address_2: String,\n",
    "                  address_3: String,\n",
    "                  city: String,\n",
    "                  state: String,\n",
    "                  zip: Long,\n",
    "                  size_in_sf: Int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val stores_df = sqlContext.read.format(\"org.apache.spark.sql.cassandra\").\n",
    "        options(Map(\"keyspace\"-> \"retail\", \"table\" -> \"stores\")).\n",
    "        load().cache()\n",
    "stores_df.registerTempTable(\"stores\")\n",
    "\n",
    "import org.apache.spark.sql.SaveMode\n",
    "\n",
    "val init_potential_fraud_per_state_df = sqlContext.sql(\"select distinct s.state, 0 as number from  stores s \"   )\n",
    "init_potential_fraud_per_state_df.write.format(\"org.apache.spark.sql.cassandra\").\n",
    "        options(Map( \"table\" -> \"potential_fraud_per_state\", \"keyspace\" -> \"retail\")).\n",
    "        mode(SaveMode.Overwrite).\n",
    "        save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RDD based on the store table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "   val stores = sc.cassandraTable(\"retail\",\"stores\").select(\"store_id\",\"address\",\n",
    "      \"address_2\",\"address_3\",\"city\",\"state\",\"zip\",\"size_in_sf\"\n",
    "    ).as(Store)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stores.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Another RDD based on receipts.  \n",
    "### No case class as it's only two columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val receipts = sc.cassandraTable(\"retail\",\"receipts_by_store_date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import scala.math.BigDecimal.RoundingMode\n",
    "\n",
    "val total_receipts_by_store = receipts.map(r => (r.getInt(\"store_id\"), r.getDecimal(\"receipt_total\").setScale(2,RoundingMode.HALF_EVEN) )  ).reduceByKey(_+_)   // Add up by store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "total_receipts_by_store take 10 foreach println"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spark-Cassandra (Scala 2.10.4)",
   "language": "scala",
   "name": "spark"
  },
  "language_info": {
   "name": "scala"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
