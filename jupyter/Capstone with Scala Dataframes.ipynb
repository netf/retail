{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.SQLContext\n",
    "import org.apache.spark.sql.functions._\n",
    "import scala.math.BigDecimal.RoundingMode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a SQL Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val sqlContext = new SQLContext(sc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now some UDFs, because Spark 1.4 does not have all of the functions we need\n",
    "The round function needs to take the type java.math.BigDecimal because that's what the dataframe created from the cassandraTable contains.  It does return the scala type through the scala implicits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val concat = udf((s1:String, s2:String) => s1 + s2)\n",
    "val round = udf((f1:java.math.BigDecimal, places:Int) => f1.setScale(places,RoundingMode.HALF_EVEN))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create dataframes on the stores and receipts_by_store_date table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val stores_df = sqlContext.read.format(\"org.apache.spark.sql.cassandra\").\n",
    "      options(Map(\"keyspace\"-> \"retail\", \"table\" -> \"stores\")).\n",
    "      load()\n",
    "      \n",
    "val receipts_by_store_date_df = sqlContext.read.format(\"org.apache.spark.sql.cassandra\").\n",
    "      options(Map(\"keyspace\"-> \"retail\", \"table\" -> \"receipts_by_store_date\")).\n",
    "      load()\n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute the sales_by_state\n",
    "1. join receipts_by_store_date to store\n",
    "2. group by state\n",
    "3. sum by receipt_total\n",
    "4. do a select to add the dummy column, rename columns, compute the region and round the totals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "    val sales_by_state_df = receipts_by_store_date_df.\n",
    "      join(stores_df, stores_df(\"store_id\") === receipts_by_store_date_df(\"store_id\")).\n",
    "      groupBy(stores_df(\"state\")).\n",
    "      sum(\"receipt_total\").\n",
    "      select(lit(\"dummy\") alias \"dummy\", col(\"state\"), concat( lit(\"US-\"), col(\"state\")) alias \"region\", round(col(\"SUM(receipt_total)\"), lit(2)) alias (\"receipts_total\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+------+--------------+\n",
      "|dummy|state|region|receipts_total|\n",
      "+-----+-----+------+--------------+\n",
      "|dummy|   MS| US-MS|    5716738.79|\n",
      "|dummy|   MT| US-MT|    1821736.04|\n",
      "|dummy|   TN| US-TN|   10310875.43|\n",
      "|dummy|   NC| US-NC|   12353711.41|\n",
      "|dummy|   ND| US-ND|    2756610.09|\n",
      "|dummy|   NH| US-NH|    1865259.78|\n",
      "|dummy|   AL| US-AL|    5518584.73|\n",
      "|dummy|   NJ| US-NJ|   11041570.96|\n",
      "|dummy|   TX| US-TX|   24554911.09|\n",
      "|dummy|   NM| US-NM|    2849598.96|\n",
      "+-----+-----+------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sales_by_state_df show 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save it to sales_by_state.  First truncate the table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><tr></tr></table>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%cql truncate retail.sales_by_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sales_by_state_df.write.\n",
    "      format(\"org.apache.spark.sql.cassandra\").\n",
    "      options(Map(\"keyspace\" -> \"retail\",\n",
    "                  \"table\" -> \"sales_by_state\")).\n",
    "      save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><tr><th>dummy</th><th>receipts_total</th><th>state</th><th>region</th></tr><tr><td>dummy</td><td>28487231.46</td><td>FL</td><td>US-FL</td></tr><tr><td>dummy</td><td>24554911.09</td><td>TX</td><td>US-TX</td></tr><tr><td>dummy</td><td>20040746.96</td><td>PA</td><td>US-PA</td></tr><tr><td>dummy</td><td>19165754.08</td><td>CA</td><td>US-CA</td></tr><tr><td>dummy</td><td>15976686.76</td><td>NY</td><td>US-NY</td></tr></table>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%cql select * from retail.sales_by_state limit 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spark 1.2.1 (Scala 2.10.4)",
   "language": "scala",
   "name": "spark"
  },
  "language_info": {
   "name": "scala"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
